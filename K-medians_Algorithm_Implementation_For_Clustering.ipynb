{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:50:53.833611Z",
     "start_time": "2019-12-02T09:50:53.817999Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "# nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from pyclustering.cluster.kmedians import kmedians\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.utils import read_sample\n",
    "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
    "from jupyterthemes import jtplot\n",
    "# jtplot.style(theme='monokai', context='notebook',\n",
    "#              ticks=True, grid=False)\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jupyterthemes import jtplot\n",
    "import operator\n",
    "# jtplot.style(theme='monokai', context='notebook',\n",
    "#              ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T01:28:20.173088Z",
     "start_time": "2019-12-02T01:28:20.088471Z"
    }
   },
   "outputs": [],
   "source": [
    "# X, y = datasets.load_iris(return_X_y=True)\n",
    "# ## slim it down to 2d,\n",
    "# X = X[:, 0:2]\n",
    "df= pd.read_csv(\"final_clustering_df.csv\")\n",
    "X = np.asarray(df.iloc[:,[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T12:57:28.888770Z",
     "start_time": "2019-12-02T12:57:28.857526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating functions\n",
    "def random_medians(dat, num_clusters):\n",
    "    index = np.random.randint(0, (len(dat)-1), size=num_clusters)\n",
    "    points = dat[index]\n",
    "    return points\n",
    "\n",
    "def euclidean_distance(dat, rand_medians):\n",
    "    dist_list = []\n",
    "    for i in range(len(rand_medians)):\n",
    "        summation = np.sum((X-rand_medians[i])**2, axis=1)\n",
    "        dist_list.append(np.sqrt(summation))\n",
    "    dist_list = np.asarray(dist_list)\n",
    "    return (dist_list)\n",
    "\n",
    "def euclidean_distance_word(dat, rand_medians,word_sim):\n",
    "    dist_list = []\n",
    "    for i in range(len(rand_medians)):\n",
    "        summation = np.sum((X-rand_medians[i])**2, axis=1)\n",
    "        summation = normalize(summation.reshape(-1,1))\n",
    "        summation = summation + (1-word_sim).reshape(-1,1)\n",
    "#         summation = word_sim\n",
    "        dist_list.append(np.sqrt(summation))\n",
    "    dist_list = np.asarray(dist_list)\n",
    "    return (dist_list)\n",
    "\n",
    "def assign_obsto_clusters(dat, dist_list):\n",
    "    clusters = []\n",
    "    assigned_clusters = np.argmin(dist_list, axis=0)\n",
    "    for i in range(len(dist_list)):\n",
    "        clusters.append(dat[assigned_clusters == i])\n",
    "#     clusters = np.asarray(clusters)\n",
    "    return (clusters)\n",
    "\n",
    "def centroid_cal(assigned_clusters):\n",
    "    centroid = []\n",
    "    for i in range(len(assigned_clusters)):\n",
    "        clust= assigned_clusters[i]\n",
    "        x,y = np.median(clust[:,0]) , np.median(clust[:,1])\n",
    "        centroid.append((x,y))\n",
    "    centroid = np.asarray(centroid)\n",
    "    return (centroid)\n",
    "\n",
    "def clust_index_find (assigned_clusters,data):\n",
    "    clust_index=[]\n",
    "    X = data\n",
    "    for i in range(len(assigned_clusters)):\n",
    "        values = np.where((X==assigned_clusters[i][:,None]).all(-1))[1]\n",
    "        clust_index.append(values)\n",
    "    return (clust_index)\n",
    "        \n",
    "def plot_clusters(assigned_clusters,centroid_old):\n",
    "    color = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    for i in range(len(assigned_clusters)):\n",
    "        clust = assigned_clusters[i]\n",
    "        plt.scatter(clust[:,0],clust[:,1],color=RS[i])\n",
    "        plt.scatter(centroid_old[i,0],centroid_old[i,1],color=color[i],\n",
    "                    label=i, marker='p', s=100, edgecolor='k')\n",
    "    plt.show()\n",
    "    \n",
    "def k_median(data,nc):\n",
    "    X=data\n",
    "    rand_medians = random_medians(X,num_clusters=nc)\n",
    "    centroid_old = rand_medians\n",
    "    dist_array = euclidean_distance(X,rand_medians)\n",
    "    assigned_clusters = assign_obsto_clusters(X,dist_array)\n",
    "    clust_index = clust_index_find(assigned_clusters,X)\n",
    "    centroids = centroid_cal(assigned_clusters)\n",
    "    i=1\n",
    "    while(np.any(centroid_old != centroids)):\n",
    "        centroid_old = centroids\n",
    "        dist_array = euclidean_distance(X,centroid_old)\n",
    "        assigned_clusters = assign_obsto_clusters(X,dist_array)\n",
    "        clust_index = clust_index_find(assigned_clusters,X)\n",
    "        centroids = centroid_cal(assigned_clusters)\n",
    "        i=i+1        \n",
    "#         print (\"old=\",centroid_old ,\"new=\",centroids)\n",
    "    return (clust_index)\n",
    "\n",
    "def similar_prod(item,no_item,data):\n",
    "    user = []\n",
    "    X = np.asarray(data.iloc[:,[1,2]])\n",
    "    user.append(X[item])\n",
    "    wordsim = word_sim_calc(data,item)\n",
    "    dist_array = euclidean_distance_word(X,user,wordsim)\n",
    "    dict = { i : dist_array[:,i] for i in range(0, dist_array.shape[1] ) }\n",
    "    sorted_dict = sorted(dict.items(), key=operator.itemgetter(1))\n",
    "    similar_prod_index = sorted_dict[0:(no_item+1)]\n",
    "    return (df.iloc[np.asarray(similar_prod_index)[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T12:57:32.584112Z",
     "start_time": "2019-12-02T12:57:32.568456Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_sim_calc(df,item):\n",
    "    file_docs = []\n",
    "    for i in range(len(df)):\n",
    "        tokens = sent_tokenize(df['Names'][i])\n",
    "        file_docs.append(tokens)\n",
    "#         for line in tokens:\n",
    "                \n",
    "#     file_docs = file_docs[0:20242]\n",
    "#     gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "#             for text in file_docs]\n",
    "#     dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "\n",
    "    mydict = corpora.Dictionary([simple_preprocess((\" \".join(line))) for line in file_docs])\n",
    "#     mydict = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [mydict.doc2bow(simple_preprocess((\" \".join(line)))) for line in file_docs]\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "#     for doc in tfidf[corpus]:\n",
    "#         print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "    workdir = r'C:\\Users\\DataMining\\Project\\ '\n",
    "    sims = gensim.similarities.Similarity(workdir,tfidf[corpus],num_features=len(dictionary))\n",
    "\n",
    "    # perform a similarity query against the corpus\n",
    "    query_doc_tf_idf = tfidf[corpus[item]]\n",
    "    # print(document_number, document_similarity)\n",
    "#     print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "    word_sim = sims[query_doc_tf_idf]\n",
    "    return (word_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T12:57:43.887695Z",
     "start_time": "2019-12-02T12:57:33.301897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select product index : 66\n",
      "Select no of similar products : 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>20 Light and Dark Blue 21st Birthday Party Bal...</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10774</th>\n",
       "      <td>Unique Party 80883 - 12\" Latex Glitz Blue 21st...</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>Birthday Party Decorations - Bunting Happy 21s...</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>Unique Party 80919 - 12\" Latex Glitz Black and...</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>Perfectly Pink Party Happy 70th Birthday Paper...</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7143</th>\n",
       "      <td>10 Light and Dark Blue 50th Birthday Party Bal...</td>\n",
       "      <td>21.741747</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Names      Price  Rating\n",
       "66     20 Light and Dark Blue 21st Birthday Party Bal...   1.740000     4.4\n",
       "10774  Unique Party 80883 - 12\" Latex Glitz Blue 21st...   1.740000     4.4\n",
       "11540  Birthday Party Decorations - Bunting Happy 21s...   1.740000     4.4\n",
       "2733   Unique Party 80919 - 12\" Latex Glitz Black and...   1.740000     4.4\n",
       "640    Perfectly Pink Party Happy 70th Birthday Paper...   1.740000     4.4\n",
       "7143   10 Light and Dark Blue 50th Birthday Party Bal...  21.741747     0.0"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = int(input(\"Select product index : \") )\n",
    "num = int(input(\"Select no of similar products : \") )\n",
    "similar_prod(ind,num,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:07:30.516803Z",
     "start_time": "2019-12-02T09:07:30.501194Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    0,  2256, 10045, ..., 19806, 20233, 20237], dtype=int64),\n",
       " array([    1,  3810, 19725, ..., 20234, 20240, 20241], dtype=int64)]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = k_median(X,5)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T04:53:06.604085Z",
     "start_time": "2019-12-01T04:53:06.487915Z"
    }
   },
   "outputs": [],
   "source": [
    "final_clustering_df.to_csv (r'C:\\Users\\DataMining\\Project\\final_clustering_df.csv', index = None, header=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 285.383666,
   "position": {
    "height": "266.383px",
    "left": "358px",
    "right": "20px",
    "top": "138px",
    "width": "373.833px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
